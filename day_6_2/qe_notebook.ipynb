{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training with all-in-one expectation method\n",
    "\n",
    "- companion notebook to the paper \"Will Artificial Intelligence Replace Computational Economists any Time Soon?\"\n",
    "\n",
    "- demonstrates how to solve a consumption savings model using one of the model exposed in the paper\n",
    "\n",
    "- we use a neural network to approximate agents decision and use euler equation to formulate a single objective, which is minimized by adjusting the weights in the n.n. (aka) training.\n",
    "\n",
    "- to formulate this objective, we use one single unified expectation operator, which allows for efficient, parallel calculations using a deep learning framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we load several libraries\n",
    "# missing library x can be installed using `pip install x`\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm         # tqdm is a nice library to visualize ongoing loops\n",
    "import datetime\n",
    "class Vector: pass\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Tensorflow](https://www.tensorflow.org/) is a deeplearning library. It can be used to train neural networks, but also to produce computational graphs (aka programs), which can be differentiated automatically, optimized and run on very scalable architecture.\n",
    "Version 2.0 introduces a new way to build graph, which allows for more intuitive graph definition, essentially by writing numpy-like code. We can install it using:\n",
    "`pip install tensorflow==2.0.0-rc1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model\n",
    "\n",
    "We consider the following version of a consumption-saving problem.\n",
    "\n",
    "There are four different stochastic processes, interest rate ($r_t$), discount factor shock ($\\delta_t$), transitory component of income ($q_t$) and permanent component of income ($p_t$). Total income is $y_t=p_t q_t$.\n",
    "All processes $r_t$, $\\delta_t$, $q_t$ and $p_t$ follow an AR1 process whose specification is given in the code.\n",
    "\n",
    "Agent consumes $c_t$, a fraction $\\zeta_t\\in[0,1]$ of disposable income $w_t$ whose evolution is given by:\n",
    "\n",
    "$$w_t = y_t + (w_{t-1}-c_{t-1}) r_t$$\n",
    "\n",
    "Given a discount parameter $\\beta\\in[0,1[$ the objective is to minimize:\n",
    "\n",
    "$$E_0 \\sum_{t\\geq0} \\delta_t \\beta^t U(c_t)$$\n",
    "\n",
    "where $U(x)=\\frac{x^{1-\\gamma}}{1-\\gamma}$ given initial state. The corresponding Euler equation is:\n",
    "\n",
    "$$ \\beta E_t \\left[ \\frac{\\delta_{t+1}}{\\delta_t}  \\frac{U(c_{t+1})}{U(c_t)} r_{t+1} \\right] \\leq 1 \\perp \\zeta_t \\leq 1$$\n",
    "\n",
    "which is by definition of the complementerity sign ($\\perp$) equivalent to:\n",
    "\n",
    "$$\\max\\left(1-\\beta E_t \\left[ \\frac{\\delta_{t+1}}{\\delta_t}  \\frac{U(c_{t+1})}{U(c_t)} r_{t+1} \\right], 1- \\zeta_t \\right)= 0$$\n",
    "\n",
    "\n",
    "The presence of expected terms within a nonlinear operator (the $max$) is a problem for our algorithm so that we reformulate the problem as finding ($h_t$ and $\\zeta_t$) which satisfy the \"optimality\" conditions:\n",
    "\n",
    "$$\\max\\left(1-h_t, 1- \\zeta_t \\right)= 0$$\n",
    "$$\\beta E_t \\left[ \\frac{\\delta_{t+1}}{\\delta_t}  \\frac{U(c_{t+1})}{U(c_t)} r_{t+1} \\right] - h_t = 0$$\n",
    "\n",
    "\n",
    "Thanks to this transformation the problem takes the form of one single expectation taken over a vector-valued non-linear function:\n",
    "\n",
    "\n",
    "$$E_t \\left[ \\left( \\begin{matrix}\n",
    "\\max\\left(1-h_t, 1- \\zeta_t \\right) \\\\\n",
    "\\beta \\frac{\\delta_{t+1}}{\\delta_t}  \\frac{U(c_{t+1})}{U(c_t)} r_{t+1} - h_t\n",
    "\\end{matrix}\\right)\\right] = \\left( \\begin{matrix}0\\\\0\\end{matrix} \\right)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# model calibration\n",
    "β = 0.9\n",
    "γ = 2.0\n",
    "σ = 0.1\n",
    "ρ = 0.9\n",
    "σ_r = 0.001\n",
    "ρ_r = 0.2\n",
    "σ_p = 0.001\n",
    "ρ_p = 0.9\n",
    "σ_q = 0.001\n",
    "ρ_q = 0.9\n",
    "σ_δ = 0.001\n",
    "ρ_δ = 0.2\n",
    "rbar = 1.04\n",
    "eps = 0.0001   # so that utility is never negative\n",
    "\n",
    "mute = 0.0     # if 1 then collapses to basic model\n",
    "# xibar = 0.95   # steady state value of ξ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ergodi\n",
    "σ_e_r = σ_r/(1-ρ_r**2)**0.5\n",
    "σ_e_p = σ_p/(1-ρ_p**2)**0.5\n",
    "σ_e_q = σ_q/(1-ρ_q**2)**0.5\n",
    "σ_e_δ = σ_δ/(1-ρ_δ**2)**0.5\n",
    "\n",
    "wmin = 0.1\n",
    "wmax = 4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Decision Rule\n",
    "\n",
    "Since the model is time-homogenous, we look for a decision rule $\\left( \\begin{matrix} \\zeta_t\\\\ h_t \\end{matrix} \\right) = \\varphi(s_t)$  where $s_t=r_t, \\delta_t, q_t, p_t, w_t$ is the 5-dimensional state-space and $\\varphi$ a function to be determined. We approximate the actual $\\varphi$ by looking in a family of functions $\\varphi(...;\\theta)$ parameterized by $\\theta$.\n",
    "\n",
    "\n",
    "In our application, this family is determined by a topology of a neural network which can be easily built with keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construction of d.r.\n",
    "\n",
    "lrelu = tf.keras.layers.LeakyReLU(alpha=0.1)\n",
    "layers = [\n",
    "    tf.keras.layers.Dense(32, activation='relu', input_dim=5, bias_initializer='he_uniform'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation=tf.keras.activations.linear)\n",
    "]\n",
    "\n",
    "perceptron = tf.keras.Sequential(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(perceptron, to_file='model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the input of the neural network is a $N.5$ matrix, where N is the number of points to be evaluated, aka the size of the mini-batch\n",
    "\n",
    "def dr(r: Vector, δ: Vector, q: Vector, p: Vector, w: Vector)-> Tuple[Vector, Vector]:\n",
    "\n",
    "    # we normalize input so as it will typically be comprised between -1 and 1\n",
    "    r = r/σ_e_r/2\n",
    "    δ = δ/σ_e_δ/2\n",
    "    q = q/σ_e_q/2\n",
    "    p = p/σ_e_p/2\n",
    "    w = (w-wmin)/(wmax-wmin)*2.0-1.0\n",
    "\n",
    "    # we prepare input to the perceptron\n",
    "    s = tf.concat([_e[:,None] for _e in [r,δ,q,p,w]], axis=1) # equivalent to np.column_stack\n",
    "\n",
    "    x = perceptron(s) # an N.2 matrix\n",
    "\n",
    "    # consumption share is always in [0,1]\n",
    "    ζ = tf.sigmoid( x[:,0] )\n",
    "    # expectation of marginal consumption is always positive\n",
    "    h = tf.exp( x[:,1] )\n",
    "    return (ζ, h)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot the initial guess (against w). Not that the coefficients of the perceptron are initialized with random values,\n",
    "# so that each run will provide a different plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wvec = np.linspace(wmin, wmax, 100)\n",
    "svec = [wvec*0]*4 + wvec\n",
    "# r,p,q,δ are zero-mean\n",
    "ζvec, hvec = dr(wvec*0, wvec*0, wvec*0, wvec*0, wvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(wvec, wvec, linestyle='--', color='black')\n",
    "plt.plot(wvec, wvec*ζvec)\n",
    "plt.xlabel(\"w_t\")\n",
    "plt.xlabel(\"c_t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that so far, using tensorflow did not depart very significantly from using numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct the objective\n",
    "\n",
    "By substituting $c_t$ and $c_{t+1}$ it is clear that Euler equation in (ref) depends ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euler_residual(r: Vector, δ: Vector, q: Vector, p: Vector, w: Vector):\n",
    "\n",
    "    # all inputs are expected to have the same size N\n",
    "    N = tf.size(r)\n",
    "\n",
    "    # arguments correspond to the values of the states today\n",
    "    ζ, h = dr(r, δ, q, p, w)\n",
    "    c = ζ*w\n",
    "\n",
    "    # transitions of the exogenous processes\n",
    "    rnext = r*ρ_r + tf.random.normal(shape=(N,), stddev=σ_r)\n",
    "    δnext = δ*ρ_δ + tf.random.normal(shape=(N,), stddev=σ_δ)\n",
    "    pnext = p*ρ_p + tf.random.normal(shape=(N,), stddev=σ_p)\n",
    "    qnext = q*ρ_q + tf.random.normal(shape=(N,), stddev=σ_q)\n",
    "\n",
    "    # transition of endogenous variables\n",
    "    wnext = tf.exp(pnext)*tf.exp(qnext) + (w-c)*rbar*tf.exp(rnext)\n",
    "\n",
    "    ζnext, hnext = dr(rnext, δnext, qnext, pnext, wnext)\n",
    "    cnext = ζnext*wnext\n",
    "    \n",
    "    res1 = β*tf.exp(δnext-δ)*(cnext)**(-γ)*rbar*tf.exp(rnext) - h\n",
    "    res2 = tf.minimum(h**(-1/γ), w) - c\n",
    "\n",
    "    return (res1, res2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euler(r: Vector, δ: Vector, q: Vector, p: Vector, w: Vector):\n",
    "\n",
    "    res1_1, res1_2 = euler_residual(r, δ, q, p, w)\n",
    "    res2_1, res2_2 = euler_residual(r, δ, q, p, w)\n",
    "\n",
    "    res = res1_1*res2_1 + res1_2*res2_2\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(r: Vector, δ: Vector, q: Vector, p: Vector, w: Vector):\n",
    "    res = euler(r, δ, q, p, w)\n",
    "    return (tf.reduce_mean(res**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_state(N):\n",
    "    r = np.random.randn(N).astype('float32')*σ_r\n",
    "    δ = np.random.randn(N).astype('float32')*σ_δ\n",
    "    q = np.random.randn(N).astype('float32')*σ_q\n",
    "    p = np.random.randn(N).astype('float32')*σ_p\n",
    "    w = wmin + np.random.rand(N).astype('float32')*(wmax-wmin)\n",
    "    return (r,δ,q,p,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1024\n",
    "s = draw_state(N)\n",
    "v = loss(*s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we have been using numpy but the result is a tensor object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: concise graph about the computation.\n",
    "Maybe screenshot of tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = perceptron.trainable_variables\n",
    "optimizer = Adam()\n",
    "# optimizer = SGD(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(k, κ=0.5):\n",
    "\n",
    "    s = draw_state(N)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        xi = loss(*s)\n",
    "\n",
    "    grads = tape.gradient(xi, variables)\n",
    "    optimizer.apply_gradients(zip(grads,variables))\n",
    "\n",
    "    return xi\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_me(K):\n",
    "\n",
    "    drs = []\n",
    "    vals = []\n",
    "    for k in tqdm(tf.range(K)):\n",
    "        val = train_step(k)\n",
    "        vals.append(val.numpy())\n",
    "    return vals, drs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with writer.as_default():\n",
    "res, drs = train_me(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.sqrt( res) )\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wvec = np.linspace(wmin, wmax, 100)\n",
    "ζvec, hvec = dr(wvec*0, wvec*0, wvec*0, wvec*0, wvec)\n",
    "\n",
    "plt.plot(wvec, wvec, linestyle='--', color='black')\n",
    "plt.plot(wvec, wvec*ζvec)\n",
    "plt.xlabel(\"w_t\")\n",
    "plt.xlabel(\"c_t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
