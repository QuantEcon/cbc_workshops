{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 0:\n",
    "\n",
    "Define 3x3 matrix `M` with biggest eigenvalue smaller than 1. \n",
    "Consider Ar1 process $X_t = M X_{t-1} + E_t$ where E_t follows a standard normal distribution.\n",
    "Simulate for T=1000 periods starting with $X_0=[0,0,0]$. Compute 1000 draws to approximate the standard deviation of the ergodic distribution.\n",
    "\n",
    "Compare the performance of different programming styles using BenchmarkTools:\n",
    "- matlab-like\n",
    "- in-place\n",
    "- using local variables\n",
    "- using static arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 1: Linear Regression\n",
    "\n",
    "1. __For $N=100$, compute a sample $(x_i, y_i)_{i=[1:N]}$ satisfying $$y_i=0.4+2.5 x_i + \\epsilon_i$$ where $x_i$ uniformly distributed between 0 and 1 and $\\epsilon_i$ is drawn from a normal distribution with standard deviation 0.5.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Define the model $f(x;a,b)=a+b x$. Find the parameters $a$ and $b$ which minimize the objective $\\xi(a,b)=\\sum_i (f(x_i;a,b)-y_i)^2$ by using a numerical optimization algorithm (not the formula for the regression). Plot.__\n",
    "\n",
    "<mark>Hint</mark>: you can write your own gradient descent algorithm or use an optimization library)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: constrained optimization\n",
    "\n",
    "Consider the function $f(x,y) = 1-(x-0.5)^2 -(y-0.3)^2$.\n",
    "\n",
    "__Use Optim.jl to minimize $f$ without constraint. Check you understand diagnostic information returned by the optimizer.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Now, consider the constraint $x<0.3$ and maximize $f$ under this new constraint.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Reformulate the problem as a root finding with lagrangians. Write the complementarity conditions.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solve using NLsolve.jl__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.1",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
